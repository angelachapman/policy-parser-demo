{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007324ed",
   "metadata": {},
   "source": [
    "## ISO Form Code Extractor\n",
    "\n",
    "This notebook extracts ISO (Insurance Services Office) form codes from commercial insurance policy PDFs using a two-stage approach:\n",
    "\n",
    "1. **Stage 1: Regex-based extraction** - Scans PDF headers and footers for ISO codes matching standard patterns (e.g., `CG 00 01 04 13`, `GL 0169 0001`)\n",
    "2. **Stage 2: LLM refinement** - Uses an LLM to verify and extract codes from pages that are missing codes or have ambiguous results\n",
    "\n",
    "### Key Features\n",
    "- **Automatic OCR fallback** for scanned/image-based PDFs\n",
    "- **Parallel LLM processing** for efficient refinement \n",
    "- Outputs codes with page numbers and text snippets for verification\n",
    "\n",
    "### Typical Workflow\n",
    "1. Extract codes using `extract_iso_codes_from_headers_footers(pdf_path)`\n",
    "2. Refine results using `refine_iso_codes_with_llm(pdf_path, initial_codes)`\n",
    "3. Optionally save results as ground truth JSON for future comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import asyncio\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Shared config and helpers\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Regex for ISO codes - simplified to capture most common patterns only\n",
    "ISO_CODE_PATTERN = re.compile(\n",
    "    r\"\\b\"\n",
    "    r\"[A-Z]{2}[\\s\\-]\"                                  # Two-letter prefix + required space or dash\n",
    "    r\"(?:\"\n",
    "        r\"\\d{2}[\\s\\-]\\d{2}[\\s\\-]\\d{2}[\\s\\-]\\d{2}\"     # Standard: CG 00 01 04 13\n",
    "        r\"|\"\n",
    "        r\"\\d{4}[\\s\\-]\\d{4}\"                            # GL format: GL 0169 0001\n",
    "    r\")\"\n",
    "    r\"\\b\"\n",
    ")\n",
    "\n",
    "def extract_text_with_ocr(pdf_path: str, page_num: int, dpi: int = 300) -> str:\n",
    "    \"\"\"Extract text from a PDF page using OCR (for scanned documents).\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_num - 1]\n",
    "    \n",
    "    # Convert page to image\n",
    "    pix = page.get_pixmap(dpi=dpi)  # Higher DPI = better OCR accuracy\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    \n",
    "    # OCR the image\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_page_text(pdf_path: str, page_num: int, ocr_threshold: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Get text from a PDF page with automatic OCR fallback for scanned documents.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        page_num: Page number (1-indexed)\n",
    "        ocr_threshold: Minimum character count to consider text extraction successful\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text from the page\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_num - 1]\n",
    "    text = page.get_text(\"text\").strip()\n",
    "    doc.close()\n",
    "    \n",
    "    # If little/no text found, assume it's scanned and use OCR\n",
    "    if len(text) < ocr_threshold:\n",
    "        print(f\"Page {page_num}: Low text content detected ({len(text)} chars), using OCR...\")\n",
    "        text = extract_text_with_ocr(pdf_path, page_num)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_header_footer_spans(\n",
    "    page: fitz.Page,\n",
    "    header_fraction: float,\n",
    "    footer_fraction: float,\n",
    ") -> List[Tuple[str, Tuple[float, float, float, float]]]:\n",
    "    \"\"\"Extract text spans from header and footer regions of a PDF page.\"\"\"\n",
    "    rect = page.rect\n",
    "    height = rect.height\n",
    "\n",
    "    header_cutoff = rect.y0 + header_fraction * height\n",
    "    footer_cutoff = rect.y1 - footer_fraction * height\n",
    "\n",
    "    spans = []\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "\n",
    "    for block in text_dict.get(\"blocks\", []):\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                text = span.get(\"text\", \"\").strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                x0, y0, x1, y1 = span[\"bbox\"]\n",
    "                in_header = y0 <= header_cutoff\n",
    "                in_footer = y1 >= footer_cutoff\n",
    "                if in_header or in_footer:\n",
    "                    spans.append((text, (x0, y0, x1, y1)))\n",
    "\n",
    "    return spans\n",
    "\n",
    "\n",
    "def build_page_to_codes_map(\n",
    "    codes_dict: Dict[str, List[Dict[str, Any]]]\n",
    ") -> Dict[int, List[str]]:\n",
    "    \"\"\"Convert a code-to-pages mapping into a page-to-codes mapping.\"\"\"\n",
    "    page_to_codes: Dict[int, List[str]] = defaultdict(list)\n",
    "    for code, occurrences in codes_dict.items():\n",
    "        for occ in occurrences:\n",
    "            page = occ[\"page\"]\n",
    "            page_to_codes[page].append(code)\n",
    "    return page_to_codes\n",
    "\n",
    "def pages_to_fix_from_initial_output(pdf_path: str, initial_codes: dict) -> list[int]:\n",
    "    \"\"\"Identify pages that are missing codes or have ambiguous multiple codes.\"\"\"\n",
    "    # page -> set(unique codes)\n",
    "    page_to_codes = defaultdict(set)\n",
    "    for code, occs in initial_codes.items():\n",
    "        for occ in occs:\n",
    "            page_to_codes[int(occ[\"page\"])].add(code)\n",
    "\n",
    "    # total pages\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        all_pages = set(range(1, len(doc) + 1))\n",
    "\n",
    "    missing_pages = sorted([p for p in all_pages if p not in page_to_codes])\n",
    "    ambiguous_pages = sorted([p for p, codeset in page_to_codes.items() if len(codeset) >= 2])\n",
    "\n",
    "    return sorted(set(missing_pages) | set(ambiguous_pages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Stage 1: regex-only extraction\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def extract_iso_codes_from_headers_footers(\n",
    "    pdf_path: str,\n",
    "    header_fraction: float = 0.12,\n",
    "    footer_fraction: float = 0.12,\n",
    "    ocr_threshold: int = 50,\n",
    ") -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"Extract ISO form codes from PDF headers and footers using regex, with OCR fallback for scanned documents.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    codes = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        for page_index, page in enumerate(doc):\n",
    "            page_num = page_index + 1\n",
    "            \n",
    "            # First, try normal text extraction\n",
    "            spans = get_header_footer_spans(\n",
    "                page,\n",
    "                header_fraction=header_fraction,\n",
    "                footer_fraction=footer_fraction,\n",
    "            )\n",
    "\n",
    "            # Check if we got meaningful text from headers/footers\n",
    "            total_text = \"\".join([text for text, _ in spans])\n",
    "            \n",
    "            if len(total_text.strip()) < ocr_threshold:\n",
    "                # Low or no text found, use OCR on full page\n",
    "                print(f\"Page {page_num}: Low header/footer text ({len(total_text)} chars), using OCR...\")\n",
    "                page_text = extract_text_with_ocr(pdf_path, page_num)\n",
    "                \n",
    "                # Apply regex to full OCR text\n",
    "                for match in ISO_CODE_PATTERN.finditer(page_text):\n",
    "                    code = \" \".join(match.group(0).split())\n",
    "                    # Get context around the match\n",
    "                    start = max(0, match.start() - 50)\n",
    "                    end = min(len(page_text), match.end() + 50)\n",
    "                    snippet = page_text[start:end].strip()\n",
    "                    \n",
    "                    codes[code].append({\n",
    "                        \"page\": page_num,\n",
    "                        \"snippet\": snippet,\n",
    "                    })\n",
    "            else:\n",
    "                # Normal extraction from header/footer spans\n",
    "                for text, _bbox in spans:\n",
    "                    for match in ISO_CODE_PATTERN.finditer(text):\n",
    "                        code = \" \".join(match.group(0).split())\n",
    "                        \n",
    "                        codes[code].append({\n",
    "                            \"page\": page_num,\n",
    "                            \"snippet\": text,\n",
    "                        })\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a713dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Stage 2: LLM cleanup for missing or ambiguous pages only\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "base_prompt = (\n",
    "            \"You are given the text of a single page from a commercial insurance policy.\\n\"\n",
    "            \"Find all ISO (Insurance Services Office) form codes present on the page. A valid code looks like: CG 20 37 12 19. Sometimes,\"\n",
    "            \"there are variants on this pattern (extra letters, different date formats, etc)\\n\"\n",
    "            \"Return all codes found, one per line. If no codes are found, return the string NONE.\\n\" \\\n",
    "            \"Do not extract policy numbers, which often start with CPP.\\n\"\n",
    "            \"If there are two very similar codes on the same page (for example, differing only by punctuation), assume they are the same.\"\n",
    "            \"Use the context to determine if the code relates to an endorsement for this policy.\\n\"\n",
    "            \"Do not extract carrier or program codes (for example, A00000)\"\n",
    "            \"The codes are often near the top or bottom of the page.\"\n",
    "        )\n",
    "\n",
    "async def refine_iso_codes_with_llm(\n",
    "    pdf_path: str,\n",
    "    initial_codes: Dict[str, List[Dict[str, Any]]],\n",
    "    model_name: str = \"gpt-5-mini\",\n",
    ") -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"Use LLM to refine ISO code extraction on pages that are missing codes or have ambiguous results.\"\"\"\n",
    "    page_to_codes = build_page_to_codes_map(initial_codes)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    num_pages = len(doc)\n",
    "\n",
    "    try:\n",
    "        all_pages = set(range(1, num_pages + 1))\n",
    "\n",
    "        pages_to_fix = pages_to_fix_from_initial_output(pdf_path, initial_codes)\n",
    "\n",
    "        print(f\"Using LLM to check pages: {pages_to_fix}\")\n",
    "\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=0.0,  # deterministic\n",
    "        )\n",
    "\n",
    "        final_page_codes = {}\n",
    "\n",
    "        # Pages already clean\n",
    "        for page in sorted(all_pages - set(pages_to_fix)):\n",
    "            final_page_codes[page] = page_to_codes[page]\n",
    "\n",
    "        # Parallelize LLM calls for all pages that need fixing\n",
    "        async def process_page(page_num: int) -> Tuple[int, List[str]]:\n",
    "            \"\"\"Process a single page with LLM to extract ISO codes.\"\"\"\n",
    "            page_text = get_page_text(pdf_path, page_num)\n",
    "            prompt = base_prompt + \"\\nPage text:\\n\" + page_text\n",
    "\n",
    "            response = await llm.ainvoke(prompt)\n",
    "            raw = getattr(response, \"content\", str(response))\n",
    "            print(f\"Page num: {page_num}, LLM raw output: {raw}\")\n",
    "            \n",
    "            # Parse multiple codes (one per line)\n",
    "            lines = [line.strip() for line in raw.strip().split('\\n') if line.strip()]\n",
    "            \n",
    "            if len(lines) == 1 and lines[0].upper() == \"NONE\":\n",
    "                return (page_num, [])\n",
    "            else:\n",
    "                # Normalize each code (collapse multiple spaces)\n",
    "                codes = [\" \".join(line.split()) for line in lines if line.upper() != \"NONE\"]\n",
    "                return (page_num, codes)\n",
    "\n",
    "        # Process all pages concurrently\n",
    "        tasks = [process_page(page_num) for page_num in pages_to_fix]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Store results\n",
    "        for page_num, codes in results:\n",
    "            final_page_codes[page_num] = codes\n",
    "\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "    # Convert from page->codes format to code->pages format (same as initial_codes)\n",
    "    result = defaultdict(list)\n",
    "    \n",
    "    for page_num, codes in sorted(final_page_codes.items()):\n",
    "        for code in codes:\n",
    "            snippet = \"\"\n",
    "            \n",
    "            # Try to find matching snippet from initial extraction\n",
    "            for orig_code, occurrences in initial_codes.items():\n",
    "                for occ in occurrences:\n",
    "                    if occ[\"page\"] == page_num and orig_code == code:\n",
    "                        snippet = occ[\"snippet\"]\n",
    "                        break\n",
    "                if snippet:\n",
    "                    break\n",
    "            \n",
    "            # If no snippet found, use \"LLM extraction\"\n",
    "            if not snippet:\n",
    "                snippet = \"LLM extraction\"\n",
    "            \n",
    "            result[code].append({\n",
    "                \"page\": page_num,\n",
    "                \"snippet\": snippet\n",
    "            })\n",
    "    \n",
    "    return dict(result)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example usage\n",
    "# -------------------------------------------------------------------\n",
    "# pdf_path = \"your_policy.pdf\"\n",
    "# initial = extract_iso_codes_from_headers_footers(pdf_path)\n",
    "# refined = refine_iso_codes_with_llm(pdf_path, initial)\n",
    "# refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"examples/utica-commercial-package-policy.pdf\"\n",
    "\n",
    "start_time = time.time()\n",
    "initial = extract_iso_codes_from_headers_footers(pdf_path)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Regex extraction took {elapsed_time:.2f} seconds, gave {len(initial)} codes\")\n",
    "initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "result = await refine_iso_codes_with_llm(pdf_path, initial, model_name=\"gpt-5-mini\") \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"LLM refinement took {elapsed_time:.2f} seconds, gave {len(result)} codes\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if results look good and have all the correct codes, we can store them as ground truth\n",
    "# leave it commented out so that we don't accidentally store results without checking them first\n",
    "\n",
    "import json\n",
    "with open(\"examples/ground_truth/michigan-hospitality-liability-forms.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da343f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code allows you to read in the json ground truth in case you want to inspect it or compare\n",
    "import json\n",
    "with open(\"examples/ground_truth/michigan-hospitality-liability-forms.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "print(f\"Ground truth has {len(ground_truth)} codes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbce4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare keys between ground_truth and result (tolerant to spaces and dashes)\n",
    "def normalize_code(code: str) -> str:\n",
    "    \"\"\"Normalize ISO code by removing spaces and dashes for comparison.\"\"\"\n",
    "    return code.replace(\" \", \"\").replace(\"-\", \"\")\n",
    "\n",
    "# Create normalized mappings\n",
    "gt_normalized = {normalize_code(k): k for k in ground_truth.keys()}\n",
    "result_normalized = {normalize_code(k): k for k in result.keys()}\n",
    "\n",
    "gt_norm_keys = set(gt_normalized.keys())\n",
    "result_norm_keys = set(result_normalized.keys())\n",
    "\n",
    "missed_norm_keys = gt_norm_keys - result_norm_keys\n",
    "extra_norm_keys = result_norm_keys - gt_norm_keys\n",
    "\n",
    "print(f\"Missed codes (in ground_truth but not in result): {len(missed_norm_keys)}\")\n",
    "for norm_key in sorted(missed_norm_keys):\n",
    "    print(f\"  - {gt_normalized[norm_key]}\")\n",
    "\n",
    "print(f\"\\nExtra codes (in result but not in ground_truth): {len(extra_norm_keys)}\")\n",
    "for norm_key in sorted(extra_norm_keys):\n",
    "    print(f\"  - {result_normalized[norm_key]}\")\n",
    "\n",
    "if not missed_norm_keys and not extra_norm_keys:\n",
    "    print(\"\\nâœ“ Perfect match! All codes match between ground_truth and result.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
